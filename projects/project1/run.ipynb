{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import  *\n",
    "from helpers import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_STATE' 'FMONTH' 'IDATE' 'IMONTH' 'IDAY' 'IYEAR' 'DISPCODE' 'SEQNO'\n",
      " '_PSU' 'CTELENUM']\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data/dataset/\", sub_sample=False)\n",
    "assert len(y_train) == len(x_train), \"Number of labels and number of rows in the dataset should be equal\"\n",
    "\n",
    "#Get column names\n",
    "column_names = np.genfromtxt(\"data/dataset/x_train.csv\", delimiter=\",\", dtype=str, max_rows=1)\n",
    "column_names = column_names[1:] #Remove Id column to match the number of features loaded\n",
    "print(column_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174 Columns have more than 1/3 % NaNs   \n"
     ]
    }
   ],
   "source": [
    "# Handling NaN values\n",
    "# We will remove columns with more than 1/3% of NaNs\n",
    "\n",
    "nan_counts = np.isnan(x_train).sum(axis=0)\n",
    "columns_to_remove = np.where(nan_counts > len(x_train) / 3)[0]\n",
    "\n",
    "print(f\"{len(columns_to_remove)} Columns have more than 1/3 % NaNs   \")\n",
    "\n",
    "data_cleaned = np.delete(x_train, columns_to_remove, axis=1)\n",
    "x_test_cleaned = np.delete(x_test, columns_to_remove, axis=1)\n",
    "\n",
    "columns_clean = np.delete(column_names, columns_to_remove)\n",
    "\n",
    "#Handling Nan Values\n",
    "# Replacing NaN with Median Values \n",
    "# More robust to Outliers & conform to categorical features\n",
    "\n",
    "medians = np.nanmedian(data_cleaned, axis=0)\n",
    "means = np.nanmean(data_cleaned, axis=0) # we will use this later\n",
    "stds = np.nanstd(data_cleaned, axis=0) # we will use this later\n",
    "\n",
    "data_cleaned[np.isnan(data_cleaned)] = np.take(medians, np.isnan(data_cleaned).nonzero()[1])\n",
    "assert np.isnan(data_cleaned).sum() == 0, \"There should be no NaNs in the dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing Highly Inter-correlated Features\n",
    "data_cleaned, columns_to_keep, highly_correlated_cols = remove_highly_correlated_columns(data_cleaned, y_train ,threshold=0.8)\n",
    "\n",
    "names_tobe_removed = columns_clean[highly_correlated_cols]\n",
    "indices_to_remove = np.where(np.isin(column_names, names_tobe_removed))[0]\n",
    "columns_to_remove = list(set(np.concatenate((columns_to_remove, indices_to_remove))))\n",
    "\n",
    "columns_clean = columns_clean[columns_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the correlation of features with the y label\n",
    "corr = np.corrcoef(data_cleaned.T, y_train)\n",
    "corr_to_y = corr[-1, :-1]\n",
    "sorted_indices = np.argsort(np.abs(corr_to_y))[::-1]\n",
    "sorted_features = columns_clean[sorted_indices]\n",
    "sorted_corr = corr_to_y[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove features with low correlation (less than threshold=0.05) with the y label\n",
    "corr_threshold = 0.05\n",
    "corr_mask = np.abs(corr_to_y) > corr_threshold\n",
    "\n",
    "data_cleaned = data_cleaned[:, corr_mask]\n",
    "\n",
    "names_tobe_removed = columns_clean[~corr_mask]\n",
    "indices_to_remove = np.where(np.isin(column_names, names_tobe_removed))[0]\n",
    "columns_to_remove = list(set(np.concatenate((columns_to_remove, indices_to_remove))))\n",
    "\n",
    "columns_clean = columns_clean[corr_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only the top 40 features with the highest correlation with the y label\n",
    "# Separate categorical and non-categorical features in different lists\n",
    "\n",
    "relevant_cat_features= [] # categorical features\n",
    "relevant_non_cat_features = [] # non-categorical features\n",
    "\n",
    "for feature in sorted_features[:40]:\n",
    "    column_idx = column_names.tolist().index(feature)\n",
    "    values = np.unique(x_train[:,column_idx])\n",
    "    if len(values) <= 8:\n",
    "        relevant_cat_features.append(feature)\n",
    "    else:\n",
    "        relevant_non_cat_features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = data_cleaned.copy() # make a copy of the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expand matrix by one hot encoding\n",
    "x_train1 = one_hot_encoder(x_train1, columns_clean, relevant_cat_features) #one hot encoding\n",
    "x_train1 = concat_features(data_cleaned, x_train1, relevant_non_cat_features, columns_clean) #concatenate non categorical features with categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13881/13881 [02:09<00:00, 106.93it/s]\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.8 # 80% of trainnig data is used for training\n",
    "seed =1 # random seed\n",
    "x_train_split, x_test_split , y_train_split, y_test_split = train_test_split(x_train1, y_train, test_size=1-ratio, random_state=seed)\n",
    "#SMOTE Oversampling\n",
    "x_train_split , y_train_split = SMOTE(x_train_split, y_train_split, k=5,ratio=0.6) #0.5\n",
    "#Undersampling\n",
    "x_train_split , y_train_split = undersample_majority(x_train_split, y_train_split,ratio=0.6) #0.85\n",
    "x_train_split, x_mean , x_std = standardize(x_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Expansion\n",
    "#Selected interactions are the interactions (i,j) that have been selected by the feature expansion function\n",
    "x_train_split, selected_interactions = feature_expansion(x_train_split, y_train_split,desired_number_of_features=130) #180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Training ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up parameters\n",
    "seed = 0 # Random seed for determinisitc results\n",
    "gamma = 0.001 # Learning rate 0.001\n",
    "max_iters = 20000 # Maximum number of iterations 20000\n",
    "_lambda = 0.01 # Regularization parameter 0.01\n",
    "\n",
    "#Prepping data for training\n",
    "y , tx = build_model_data( y_train_split,x_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.8605029248246178 (with regularization)\n",
      "Current iteration=100, loss=0.6083841700793691 (with regularization)\n",
      "Current iteration=200, loss=0.5317004121196145 (with regularization)\n",
      "Current iteration=300, loss=0.4934058114787415 (with regularization)\n",
      "Current iteration=400, loss=0.4686650564892864 (with regularization)\n",
      "Current iteration=500, loss=0.45063151803567686 (with regularization)\n",
      "Current iteration=600, loss=0.43662251216024744 (with regularization)\n",
      "Current iteration=700, loss=0.425314123465999 (with regularization)\n",
      "Current iteration=800, loss=0.4159439810998501 (with regularization)\n",
      "Current iteration=900, loss=0.40802657016091703 (with regularization)\n",
      "Current iteration=1000, loss=0.40123191986114864 (with regularization)\n",
      "Current iteration=1100, loss=0.3953258304770051 (with regularization)\n",
      "Current iteration=1200, loss=0.3901368309763934 (with regularization)\n",
      "Current iteration=1300, loss=0.3855362976905621 (with regularization)\n",
      "Current iteration=1400, loss=0.38142576341064643 (with regularization)\n",
      "Current iteration=1500, loss=0.3777284797740661 (with regularization)\n",
      "Current iteration=1600, loss=0.37438364296727256 (with regularization)\n",
      "Current iteration=1700, loss=0.3713423549374972 (with regularization)\n",
      "Current iteration=1800, loss=0.3685647480651744 (with regularization)\n",
      "Current iteration=1900, loss=0.3660179064912856 (with regularization)\n",
      "Current iteration=2000, loss=0.3636743421768133 (with regularization)\n",
      "Current iteration=2100, loss=0.3615108627488108 (with regularization)\n",
      "Current iteration=2200, loss=0.3595077195574902 (with regularization)\n",
      "Current iteration=2300, loss=0.35764795849425424 (with regularization)\n",
      "Current iteration=2400, loss=0.355916919168843 (with regularization)\n",
      "Current iteration=2500, loss=0.354301843823418 (with regularization)\n",
      "Current iteration=2600, loss=0.35279156829094155 (with regularization)\n",
      "Current iteration=2700, loss=0.3513762749535175 (with regularization)\n",
      "Current iteration=2800, loss=0.35004729305863325 (with regularization)\n",
      "Current iteration=2900, loss=0.34879693560021935 (with regularization)\n",
      "Current iteration=3000, loss=0.3476183647362784 (with regularization)\n",
      "Current iteration=3100, loss=0.3465054797166251 (with regularization)\n",
      "Current iteration=3200, loss=0.34545282275481953 (with regularization)\n",
      "Current iteration=3300, loss=0.3444554993522211 (with regularization)\n",
      "Current iteration=3400, loss=0.34350911037769705 (with regularization)\n",
      "Current iteration=3500, loss=0.3426096938006243 (with regularization)\n",
      "Current iteration=3600, loss=0.3417536744219861 (with regularization)\n",
      "Current iteration=3700, loss=0.34093782028772146 (with regularization)\n",
      "Current iteration=3800, loss=0.34015920472817934 (with regularization)\n",
      "Current iteration=3900, loss=0.3394151731679931 (with regularization)\n",
      "Current iteration=4000, loss=0.3387033140068012 (with regularization)\n",
      "Current iteration=4100, loss=0.3380214329938964 (with regularization)\n",
      "Current iteration=4200, loss=0.33736753061713926 (with regularization)\n",
      "Current iteration=4300, loss=0.33673978210426375 (with regularization)\n",
      "Current iteration=4400, loss=0.33613651969750646 (with regularization)\n",
      "Current iteration=4500, loss=0.3355562169136061 (with regularization)\n",
      "Current iteration=4600, loss=0.33499747454320267 (with regularization)\n",
      "Current iteration=4700, loss=0.3344590081783979 (with regularization)\n",
      "Current iteration=4800, loss=0.3339396370862104 (with regularization)\n",
      "Current iteration=4900, loss=0.33343827426998734 (with regularization)\n",
      "Current iteration=5000, loss=0.3329539175813946 (with regularization)\n",
      "Current iteration=5100, loss=0.3324856417631015 (with regularization)\n",
      "Current iteration=5200, loss=0.3320325913172256 (with regularization)\n",
      "Current iteration=5300, loss=0.33159397410745234 (with regularization)\n",
      "Current iteration=5400, loss=0.33116905561383536 (with regularization)\n",
      "Current iteration=5500, loss=0.33075715376889075 (with regularization)\n",
      "Current iteration=5600, loss=0.3303576343119536 (with regularization)\n",
      "Current iteration=5700, loss=0.32996990660605424 (with regularization)\n",
      "Current iteration=5800, loss=0.32959341986794133 (with regularization)\n",
      "Current iteration=5900, loss=0.329227659767475 (with regularization)\n",
      "Current iteration=6000, loss=0.32887214535751336 (with regularization)\n",
      "Current iteration=6100, loss=0.32852642629974876 (with regularization)\n",
      "Current iteration=6200, loss=0.3281900803557536 (with regularization)\n",
      "Current iteration=6300, loss=0.32786271111586845 (with regularization)\n",
      "Current iteration=6400, loss=0.32754394594153763 (with regularization)\n",
      "Current iteration=6500, loss=0.32723343409933714 (with regularization)\n",
      "Current iteration=6600, loss=0.32693084506727627 (with regularization)\n",
      "Current iteration=6700, loss=0.3266358669960248 (with regularization)\n",
      "Current iteration=6800, loss=0.32634820530956504 (with regularization)\n",
      "Current iteration=6900, loss=0.32606758143139963 (with regularization)\n",
      "Current iteration=7000, loss=0.32579373162390607 (with regularization)\n",
      "Current iteration=7100, loss=0.3255264059297249 (with regularization)\n",
      "Current iteration=7200, loss=0.3252653672052238 (with regularization)\n",
      "Current iteration=7300, loss=0.32501039023710776 (with regularization)\n",
      "Current iteration=7400, loss=0.32476126093417046 (with regularization)\n",
      "Current iteration=7500, loss=0.32451777558699485 (with regularization)\n",
      "Current iteration=7600, loss=0.32427974018914807 (with regularization)\n",
      "Current iteration=7700, loss=0.3240469698140713 (with regularization)\n",
      "Current iteration=7800, loss=0.3238192880424466 (with regularization)\n",
      "Current iteration=7900, loss=0.3235965264353511 (with regularization)\n",
      "Current iteration=8000, loss=0.32337852404897494 (with regularization)\n",
      "Current iteration=8100, loss=0.3231651269871005 (with regularization)\n",
      "Current iteration=8200, loss=0.3229561879879156 (with regularization)\n",
      "Current iteration=8300, loss=0.3227515660420731 (with regularization)\n",
      "Current iteration=8400, loss=0.322551126039209 (with regularization)\n",
      "Current iteration=8500, loss=0.32235473844040397 (with regularization)\n",
      "Current iteration=8600, loss=0.32216227897431793 (with regularization)\n",
      "Current iteration=8700, loss=0.3219736283549438 (with regularization)\n",
      "Current iteration=8800, loss=0.3217886720191253 (with regularization)\n",
      "Current iteration=8900, loss=0.3216072998821598 (with regularization)\n",
      "Current iteration=9000, loss=0.3214294061099657 (with regularization)\n",
      "Current iteration=9100, loss=0.3212548889064377 (with regularization)\n",
      "Current iteration=9200, loss=0.32108365031474206 (with regularization)\n",
      "Current iteration=9300, loss=0.32091559603141895 (with regularization)\n",
      "Current iteration=9400, loss=0.3207506352322641 (with regularization)\n",
      "Current iteration=9500, loss=0.32058868040905747 (with regularization)\n",
      "Current iteration=9600, loss=0.32042964721628914 (with regularization)\n",
      "Current iteration=9700, loss=0.32027345432711035 (with regularization)\n",
      "Current iteration=9800, loss=0.3201200232978088 (with regularization)\n",
      "Current iteration=9900, loss=0.31996927844016426 (with regularization)\n",
      "Current iteration=10000, loss=0.31982114670110523 (with regularization)\n",
      "Current iteration=10100, loss=0.31967555754912996 (with regularization)\n",
      "Current iteration=10200, loss=0.31953244286700666 (with regularization)\n",
      "Current iteration=10300, loss=0.3193917368503086 (with regularization)\n",
      "Current iteration=10400, loss=0.3192533759113754 (with regularization)\n",
      "Current iteration=10500, loss=0.3191172985883294 (with regularization)\n",
      "Current iteration=10600, loss=0.31898344545880253 (with regularization)\n",
      "Current iteration=10700, loss=0.3188517590580638 (with regularization)\n",
      "Current iteration=10800, loss=0.3187221838012562 (with regularization)\n",
      "Current iteration=10900, loss=0.3185946659094781 (with regularization)\n",
      "Current iteration=11000, loss=0.3184691533394684 (with regularization)\n",
      "Current iteration=11100, loss=0.3183455957166663 (with regularization)\n",
      "Current iteration=11200, loss=0.31822394427144185 (with regularization)\n",
      "Current iteration=11300, loss=0.31810415177830464 (with regularization)\n",
      "Current iteration=11400, loss=0.3179861724979136 (with regularization)\n",
      "Current iteration=11500, loss=0.3178699621217247 (with regularization)\n",
      "Current iteration=11600, loss=0.3177554777191236 (with regularization)\n",
      "Current iteration=11700, loss=0.31764267768690513 (with regularization)\n",
      "Current iteration=11800, loss=0.3175315217009662 (with regularization)\n",
      "Current iteration=11900, loss=0.31742197067009287 (with regularization)\n",
      "Current iteration=12000, loss=0.31731398669172683 (with regularization)\n",
      "Current iteration=12100, loss=0.31720753300960797 (with regularization)\n",
      "Current iteration=12200, loss=0.3171025739731929 (with regularization)\n",
      "Current iteration=12300, loss=0.31699907499876 (with regularization)\n",
      "Current iteration=12400, loss=0.3168970025321125 (with regularization)\n",
      "Current iteration=12500, loss=0.31679632401280255 (with regularization)\n",
      "Current iteration=12600, loss=0.31669700783979843 (with regularization)\n",
      "Current iteration=12700, loss=0.3165990233385267 (with regularization)\n",
      "Current iteration=12800, loss=0.31650234072922157 (with regularization)\n",
      "Current iteration=12900, loss=0.3164069310965202 (with regularization)\n",
      "Current iteration=13000, loss=0.316312766360246 (with regularization)\n",
      "Current iteration=13100, loss=0.31621981924732273 (with regularization)\n",
      "Current iteration=13200, loss=0.31612806326477066 (with regularization)\n",
      "Current iteration=13300, loss=0.31603747267373367 (with regularization)\n",
      "Current iteration=13400, loss=0.3159480224644917 (with regularization)\n",
      "Current iteration=13500, loss=0.3158596883324156 (with regularization)\n",
      "Current iteration=13600, loss=0.3157724466548208 (with regularization)\n",
      "Current iteration=13700, loss=0.31568627446868563 (with regularization)\n",
      "Current iteration=13800, loss=0.3156011494491904 (with regularization)\n",
      "Current iteration=13900, loss=0.3155170498890489 (with regularization)\n",
      "Current iteration=14000, loss=0.3154339546785939 (with regularization)\n",
      "Current iteration=14100, loss=0.3153518432865888 (with regularization)\n",
      "Current iteration=14200, loss=0.31527069574173205 (with regularization)\n",
      "Current iteration=14300, loss=0.31519049261482895 (with regularization)\n",
      "Current iteration=14400, loss=0.3151112150016014 (with regularization)\n",
      "Current iteration=14500, loss=0.3150328445061102 (with regularization)\n",
      "Current iteration=14600, loss=0.3149553632247666 (with regularization)\n",
      "Current iteration=14700, loss=0.31487875373090773 (with regularization)\n",
      "Current iteration=14800, loss=0.3148029990599146 (with regularization)\n",
      "Current iteration=14900, loss=0.3147280826948509 (with regularization)\n",
      "Current iteration=15000, loss=0.3146539885526031 (with regularization)\n",
      "Current iteration=15100, loss=0.31458070097050056 (with regularization)\n",
      "Current iteration=15200, loss=0.314508204693399 (with regularization)\n",
      "Current iteration=15300, loss=0.31443648486120807 (with regularization)\n",
      "Current iteration=15400, loss=0.31436552699684744 (with regularization)\n",
      "Current iteration=15500, loss=0.3142953169946128 (with regularization)\n",
      "Current iteration=15600, loss=0.3142258411089398 (with regularization)\n",
      "Current iteration=15700, loss=0.31415708594354647 (with regularization)\n",
      "Current iteration=15800, loss=0.3140890384409443 (with regularization)\n",
      "Current iteration=15900, loss=0.31402168587230156 (with regularization)\n",
      "Current iteration=16000, loss=0.3139550158276456 (with regularization)\n",
      "Current iteration=16100, loss=0.3138890162063942 (with regularization)\n",
      "Current iteration=16200, loss=0.3138236752082002 (with regularization)\n",
      "Current iteration=16300, loss=0.3137589813241015 (with regularization)\n",
      "Current iteration=16400, loss=0.31369492332796206 (with regularization)\n",
      "Current iteration=16500, loss=0.3136314902681964 (with regularization)\n",
      "Current iteration=16600, loss=0.31356867145976414 (with regularization)\n",
      "Current iteration=16700, loss=0.3135064564764279 (with regularization)\n",
      "Current iteration=16800, loss=0.3134448351432628 (with regularization)\n",
      "Current iteration=16900, loss=0.3133837975294096 (with regularization)\n",
      "Current iteration=17000, loss=0.31332333394106243 (with regularization)\n",
      "Current iteration=17100, loss=0.3132634349146836 (with regularization)\n",
      "Current iteration=17200, loss=0.3132040912104353 (with regularization)\n",
      "Current iteration=17300, loss=0.3131452938058237 (with regularization)\n",
      "Current iteration=17400, loss=0.313087033889545 (with regularization)\n",
      "Current iteration=17500, loss=0.31302930285552794 (with regularization)\n",
      "Current iteration=17600, loss=0.31297209229716566 (with regularization)\n",
      "Current iteration=17700, loss=0.3129153940017298 (with regularization)\n",
      "Current iteration=17800, loss=0.3128591999449613 (with regularization)\n",
      "Current iteration=17900, loss=0.3128035022858314 (with regularization)\n",
      "Current iteration=18000, loss=0.31274829336146637 (with regularization)\n",
      "Current iteration=18100, loss=0.31269356568223095 (with regularization)\n",
      "Current iteration=18200, loss=0.3126393119269657 (with regularization)\n",
      "Current iteration=18300, loss=0.31258552493837155 (with regularization)\n",
      "Current iteration=18400, loss=0.3125321977185376 (with regularization)\n",
      "Current iteration=18500, loss=0.31247932342460616 (with regularization)\n",
      "Current iteration=18600, loss=0.3124268953645727 (with regularization)\n",
      "Current iteration=18700, loss=0.31237490699321285 (with regularization)\n",
      "Current iteration=18800, loss=0.312323351908135 (with regularization)\n",
      "Current iteration=18900, loss=0.312272223845953 (with regularization)\n",
      "Current iteration=19000, loss=0.3122215166785747 (with regularization)\n",
      "Current iteration=19100, loss=0.3121712244096035 (with regularization)\n",
      "Current iteration=19200, loss=0.31212134117084783 (with regularization)\n",
      "Current iteration=19300, loss=0.3120718612189362 (with regularization)\n",
      "Current iteration=19400, loss=0.31202277893203384 (with regularization)\n",
      "Current iteration=19500, loss=0.31197408880665645 (with regularization)\n",
      "Current iteration=19600, loss=0.31192578545458055 (with regularization)\n",
      "Current iteration=19700, loss=0.3118778635998442 (with regularization)\n",
      "Current iteration=19800, loss=0.3118303180758377 (with regularization)\n",
      "Current iteration=19900, loss=0.31178314382247896 (with regularization)\n"
     ]
    }
   ],
   "source": [
    "y[y==-1] = 0 # replace -1 with 0 \n",
    "\n",
    "w = np.random.normal(0, 0.1, tx.shape[1])\n",
    "\n",
    "weights = np.unique(y, return_counts=True)[1]*[1,2]/len(y)\n",
    "w,loss = weighted_reg_logistic_regression(y, tx, _lambda, w, max_iters, gamma, weights)\n",
    "\n",
    "#w,loss = reg_logistic_regression(y, tx, _lambda, w, max_iters, gamma)\n",
    "#w,loss = ridge_regression(y, tx, _lambda)\n",
    "#w,loss = least_squares(y, tx)\n",
    "#w,loss = mean_squared_error_gd(y, tx, w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "Accuracy:  0.7175685943436049\n",
      "Recall:  0.6222444348389885\n",
      "Precision:  0.8935485122395934\n",
      "F1 score:  0.7336168044208748\n",
      "Confusion Matrix: \n",
      " [[19465  2744]\n",
      " [13983 23033]]\n",
      "\n",
      "Test set:\n",
      "Accuracy:  0.8492670587876756\n",
      "Recall:  0.5967465753424658\n",
      "Precision:  0.31618581019778624\n",
      "F1 score:  0.41335547384651883\n",
      "Confusion Matrix: \n",
      " [[52249  7537]\n",
      " [ 2355  3485]]\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(tx, w) #predict outputs values in -1,1\n",
    "\n",
    "tx_test = (x_test_split - x_mean) / x_std #standardize test data\n",
    "selected_interaction_terms = np.column_stack([tx_test[:, i] * tx_test[:, j] for i, j in selected_interactions])\n",
    "tx_test = np.column_stack((tx_test, selected_interaction_terms))\n",
    "tx_test = np.c_[np.ones((tx_test.shape[0], 1)), tx_test] #add bias term\n",
    "\n",
    "pred_test = predict(tx_test, w)\n",
    "\n",
    "y_test_split[y_test_split==0] = -1    # Reconvert labels to -1,1\n",
    "y_train_split[y_train_split==0] = -1  # Reconvert labels to -1,1\n",
    "\n",
    "# Print accuracy, recall, precision, and f1 score for train set\n",
    "print(\"Train set:\")\n",
    "print_results(y_train_split, pred_train)\n",
    "\n",
    "# Print accuracy, recall, precision, and f1 score for test set\n",
    "print(\"\\nTest set:\")\n",
    "print_results(y_test_split, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Predicting labels for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109379, 321)\n",
      "(109379, 236)\n"
     ]
    }
   ],
   "source": [
    "#Preparing test data\n",
    "x_test1 = x_test.copy()\n",
    "x_test1 = prepare_data(x_test1, columns_to_remove) #clean data\n",
    "\n",
    "x_test_hot = one_hot_encoder(x_test1, columns_clean, relevant_cat_features) #one hot encoding\n",
    "x_test1 = concat_features(x_test1, x_test_hot, relevant_non_cat_features, columns_clean) #concatenate non categorical features with categorical features\n",
    "\n",
    "x_test1 = (x_test1 - x_mean) / x_std #standardize test data\n",
    "\n",
    "#The next two lines perform feature expansion on the test data\n",
    "selected_interaction_terms = np.column_stack([x_test1[:, i] * x_test1[:, j] for i, j in selected_interactions])\n",
    "x_test1 = np.column_stack((x_test1, selected_interaction_terms))\n",
    "\n",
    "\n",
    "x_test1  = np.c_[np.ones(x_test1.shape[0]), x_test1] #add bias term\n",
    "pred_test = predict(x_test1, w, threshold=0.62)\n",
    "\n",
    "create_csv_submission(test_ids, pred_test, \"submission_ai_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
