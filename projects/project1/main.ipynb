{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import  *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the parameters\n",
    "max_iters = 10000\n",
    "gamma = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_mse(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    n = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return -1/ n * tx.T.dot(e) \n",
    "\n",
    "\n",
    "def compute_loss_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss using mse.\"\"\"\n",
    "    n = len(y)\n",
    "    e = y - tx.dot(w)\n",
    "    return  1 / (2 * n) * np.sum(e ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_sgd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculate the loss using mse\"\"\"\n",
    "    w = initial_w\n",
    "    loss = compute_loss_mse(y, tx, w)  \n",
    "    for iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            gradient = compute_gradient_mse(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma/2 * gradient\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=compute_loss_mse(y, tx, w)))\n",
    "    loss = compute_loss_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_mse(y, tx, w):\n",
    "    N = len(y)\n",
    "    error = y - tx.dot(w)\n",
    "    gradient = -1/N * tx.T.dot(error)\n",
    "    return gradient\n",
    "\n",
    "def compute_loss_mse(y, tx, w):\n",
    "    N = len(y)\n",
    "    squared_error = (y - tx.dot(w))**2\n",
    "    loss = 1/(2*N) * np.sum(squared_error)\n",
    "    return loss\n",
    "\n",
    "def mean_squared_error_sgd(y, tx, initial_w, max_iters, gamma):\n",
    "    if max_iters == 0:\n",
    "        return initial_w, compute_loss_mse(y, tx, initial_w)\n",
    "    w = initial_w\n",
    "    for iter in range(max_iters):\n",
    "        gradient = compute_gradient_mse(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        loss = compute_loss_mse(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0630575 0.39208  ]\n",
      "0.8445947735940318\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERS = 2\n",
    "GAMMA = 0.1\n",
    "initial_w = np.array([0.5, 1.0])\n",
    "y = np.array([0.1, 0.3, 0.5])\n",
    "tx = np.array([[2.3, 3.2], [1.0, 0.1], [1.4, 2.3]])\n",
    "w, loss = mean_squared_error_sgd(\n",
    "    y[:1], tx[:1], initial_w, MAX_ITERS, GAMMA    )\n",
    "\n",
    "print( w )\n",
    "print( loss )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print( w )\n",
    "print( loss )\n",
    "expected_loss = 0.844595\n",
    "expected_w = np.array([0.063058, 0.39208])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data(\"data/dataset/x_train.csv\", sub_sample=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.loadtxt(\"data/dataset/y_train.csv\",skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299160 28975 328135\n"
     ]
    }
   ],
   "source": [
    "assert len(y) == len(yb)\n",
    "print( len(y[y == 0]), len(y[y == 1]),  len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yacine/Desktop/ML_course/projects/project1/helpers.py:47: RuntimeWarning: invalid value encountered in divide\n",
      "  x = x / std_x\n"
     ]
    }
   ],
   "source": [
    "#Replacing NAN values by the mean\n",
    "col_means = np.nanmean(input_data, axis=0) # Calculate the mean of each column\n",
    "nan_indices = np.where(np.isnan(input_data)) # Find the indices of NaN values in input_data\n",
    "input_data[nan_indices] = np.take(col_means, nan_indices[1]) # Replace NaN values with the corresponding column mean\n",
    "\n",
    "input_data , input_data_mean , input_data_std = standardize(input_data)\n",
    "input_data[np.isnan(input_data)] = 0\n",
    "\n",
    "\n",
    "# Verify that there are no more NaN values in input_data\n",
    "assert np.isnan(input_data).sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8 # 80% of data is used for training\n",
    "seed = 1 # Random seed\n",
    "\n",
    "y , tx = build_model_data(y, input_data)\n",
    "tx_train, tx_test, yb_train, yb_test = train_test_split(tx, y, test_size=1-ratio, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=181.50393598144126\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m initial_w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(tx\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m w,loss \u001b[39m=\u001b[39m mean_squared_error_gd(yb_train, tx_train, initial_w , max_iters, gamma)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m( \u001b[39m\"\u001b[39m\u001b[39mAccuracy for training Set : \u001b[39m\u001b[39m\"\u001b[39m,accuracy(yb_train, predict(tx_train,w))) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m( \u001b[39m\"\u001b[39m\u001b[39mAccuracy for testing Set : \u001b[39m\u001b[39m\"\u001b[39m,accuracy(yb_test, predict(tx_test,w)))\n",
      "File \u001b[0;32m~/Desktop/ML_course/projects/project1/implementations.py:21\u001b[0m, in \u001b[0;36mmean_squared_error_gd\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[39m=\u001b[39m compute_loss_mse(y, tx, w)\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[0;32m---> 21\u001b[0m     gradient \u001b[39m=\u001b[39m compute_gradient_mse(y, tx, w)\n\u001b[1;32m     22\u001b[0m     w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39m gamma \u001b[39m*\u001b[39m gradient\n\u001b[1;32m     23\u001b[0m     loss \u001b[39m=\u001b[39m compute_loss_mse(y, tx, w)\n",
      "File \u001b[0;32m~/Desktop/ML_course/projects/project1/implementations.py:7\u001b[0m, in \u001b[0;36mcompute_gradient_mse\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute the gradient.\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m e \u001b[39m=\u001b[39m y \u001b[39m-\u001b[39m tx\u001b[39m.\u001b[39mdot(w)\n\u001b[0;32m----> 7\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mtx\u001b[39m.\u001b[39;49mT\u001b[39m.\u001b[39;49mdot(e) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(e)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ratio = 0.8 # 80% of data is used for training\n",
    "seed = 1 # Random seed\n",
    "\n",
    "y , tx = build_model_data(y, input_data)\n",
    "tx_train, tx_test, yb_train, yb_test = train_test_split(tx, y, test_size=1-ratio, random_state=seed)\n",
    "initial_w = np.random.rand(tx.shape[1])\n",
    "w,loss = mean_squared_error_gd(yb_train, tx_train, initial_w , max_iters, gamma)\n",
    "print( \"Accuracy for training Set : \",accuracy(yb_train, predict(tx_train,w))) \n",
    "print( \"Accuracy for testing Set : \",accuracy(yb_test, predict(tx_test,w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gammma= 0.01\n",
      "Current iteration=0, loss=201.49658396920753\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m initial_w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(tx\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m w,loss \u001b[39m=\u001b[39m mean_squared_error_sgd(yb_train, tx_train, initial_w , max_iters, gamma)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m( \u001b[39m\"\u001b[39m\u001b[39mAccuracy for training Set : \u001b[39m\u001b[39m\"\u001b[39m,accuracy(yb_train, predict(tx_train,w))) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m( \u001b[39m\"\u001b[39m\u001b[39mAccuracy for testing Set : \u001b[39m\u001b[39m\"\u001b[39m,accuracy(yb_test, predict(tx_test,w)))\n",
      "\u001b[1;32m/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m w \u001b[39m=\u001b[39m initial_w\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m minibatch_y, minibatch_tx \u001b[39min\u001b[39;00m batch_iter(y, tx, \u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         gradient \u001b[39m=\u001b[39m compute_gradient_mse(minibatch_y, minibatch_tx, w)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39m gamma \u001b[39m*\u001b[39m gradient\n",
      "File \u001b[0;32m~/Desktop/ML_course/projects/project1/implementations.py:45\u001b[0m, in \u001b[0;36mbatch_iter\u001b[0;34m(y, tx, batch_size, num_batches, shuffle)\u001b[0m\n\u001b[1;32m     43\u001b[0m     shuffle_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mpermutation(np\u001b[39m.\u001b[39marange(data_size))\n\u001b[1;32m     44\u001b[0m     shuffled_y \u001b[39m=\u001b[39m y[shuffle_indices]\n\u001b[0;32m---> 45\u001b[0m     shuffled_tx \u001b[39m=\u001b[39m tx[shuffle_indices]\n\u001b[1;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     shuffled_y \u001b[39m=\u001b[39m y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_w = np.random.rand(tx.shape[1])\n",
    "w,loss = mean_squared_error_sgd(yb_train, tx_train, initial_w , max_iters, gamma)\n",
    "print( \"Accuracy for training Set : \",accuracy(yb_train, predict(tx_train,w))) \n",
    "print( \"Accuracy for testing Set : \",accuracy(yb_test, predict(tx_test,w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for training Set :  0.4540661617931644\n",
      "Accuracy for testing Set :  0.4520090816279885\n",
      "loss :  0.03574128202122788\n"
     ]
    }
   ],
   "source": [
    "w,loss = ridge_regression(yb_train, tx_train)\n",
    "print( \"Accuracy for training Set : \",accuracy(yb_train, predict(tx_train,w))) \n",
    "print( \"Accuracy for testing Set : \",accuracy(yb_test, predict(tx_test,w)))\n",
    "print(\"loss : \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=4.267883700117027\n",
      "Current iteration=100, loss=4.0022995738192195\n",
      "Current iteration=200, loss=3.4019519203669732\n",
      "Current iteration=300, loss=2.9529385407918016\n",
      "Current iteration=400, loss=2.6384217945654984\n",
      "Current iteration=500, loss=2.3432850169478554\n",
      "Current iteration=600, loss=2.061785440003834\n",
      "Current iteration=700, loss=1.7998061535052203\n",
      "Current iteration=800, loss=1.5639581192246106\n",
      "Current iteration=900, loss=1.358551117284131\n",
      "Current iteration=1000, loss=1.185171361167158\n",
      "Current iteration=1100, loss=1.0391455132882457\n",
      "Current iteration=1200, loss=0.9169948639406573\n",
      "Current iteration=1300, loss=0.8151915335270066\n",
      "Current iteration=1400, loss=0.7307573589977132\n",
      "Current iteration=1500, loss=0.660711872379133\n",
      "Current iteration=1600, loss=0.6027066198096706\n",
      "Current iteration=1700, loss=0.554470337524913\n",
      "Current iteration=1800, loss=0.5143044743501035\n",
      "Current iteration=1900, loss=0.4806511814643955\n",
      "Current iteration=2000, loss=0.4522744143106638\n",
      "Current iteration=2100, loss=0.4282587851755532\n",
      "Current iteration=2200, loss=0.40775753674286297\n",
      "Current iteration=2300, loss=0.3901125794943181\n",
      "Current iteration=2400, loss=0.37484741827905\n",
      "Current iteration=2500, loss=0.3615733975082244\n",
      "Current iteration=2600, loss=0.34999534599415055\n",
      "Current iteration=2700, loss=0.33983838495318225\n",
      "Current iteration=2800, loss=0.330913403795781\n",
      "Current iteration=2900, loss=0.323043114349049\n",
      "Current iteration=3000, loss=0.31612360466585393\n",
      "Current iteration=3100, loss=0.3100423061601284\n",
      "Current iteration=3200, loss=0.3046665196287224\n",
      "Current iteration=3300, loss=0.2999155804636217\n",
      "Current iteration=3400, loss=0.2957047094220461\n",
      "Current iteration=3500, loss=0.2919383377197649\n",
      "Current iteration=3600, loss=0.28855600901190404\n",
      "Current iteration=3700, loss=0.28550667840874244\n",
      "Current iteration=3800, loss=0.2827419301324728\n",
      "Current iteration=3900, loss=0.28022882971437985\n",
      "Current iteration=4000, loss=0.27792218846569194\n",
      "Current iteration=4100, loss=0.275781760612479\n",
      "Current iteration=4200, loss=0.2737954648918341\n",
      "Current iteration=4300, loss=0.2719264509300053\n",
      "Current iteration=4400, loss=0.27015702353123955\n",
      "Current iteration=4500, loss=0.26846902867541506\n",
      "Current iteration=4600, loss=0.26684932142046675\n",
      "Current iteration=4700, loss=0.2653112556845664\n",
      "Current iteration=4800, loss=0.263840347024656\n",
      "Current iteration=4900, loss=0.2624352568645666\n",
      "Current iteration=5000, loss=0.2610938714641428\n",
      "Current iteration=5100, loss=0.2598062306460368\n",
      "Current iteration=5200, loss=0.2585618584885028\n",
      "Current iteration=5300, loss=0.2573703876246126\n",
      "Current iteration=5400, loss=0.2562256345665818\n",
      "Current iteration=5500, loss=0.2551262326849293\n",
      "Current iteration=5600, loss=0.25405807635925165\n",
      "Current iteration=5700, loss=0.25302825188829564\n",
      "Current iteration=5800, loss=0.2520497036893742\n",
      "Current iteration=5900, loss=0.25112232895218295\n",
      "Current iteration=6000, loss=0.2502456008173249\n",
      "Current iteration=6100, loss=0.2494193750961292\n",
      "Current iteration=6200, loss=0.2486417280672503\n",
      "Current iteration=6300, loss=0.24790981063778517\n",
      "Current iteration=6400, loss=0.24721733079173805\n",
      "Current iteration=6500, loss=0.2465594527520945\n",
      "Current iteration=6600, loss=0.24593912504757084\n",
      "Current iteration=6700, loss=0.2453529812522923\n",
      "Current iteration=6800, loss=0.244800938182072\n",
      "Current iteration=6900, loss=0.24427940573628956\n",
      "Current iteration=7000, loss=0.24378806266487907\n",
      "Current iteration=7100, loss=0.2433238953888018\n",
      "Current iteration=7200, loss=0.24287819262084742\n",
      "Current iteration=7300, loss=0.2424482466047469\n",
      "Current iteration=7400, loss=0.2420290377766081\n",
      "Current iteration=7500, loss=0.24162398583667488\n",
      "Current iteration=7600, loss=0.2412260100246243\n",
      "Current iteration=7700, loss=0.24082768334837343\n",
      "Current iteration=7800, loss=0.2404383827570086\n",
      "Current iteration=7900, loss=0.24005668861537302\n",
      "Current iteration=8000, loss=0.2396812031026024\n",
      "Current iteration=8100, loss=0.23930430317452375\n",
      "Current iteration=8200, loss=0.2389305177709421\n",
      "Current iteration=8300, loss=0.23856790010187337\n",
      "Current iteration=8400, loss=0.23821486523989488\n",
      "Current iteration=8500, loss=0.23787055760166612\n",
      "Current iteration=8600, loss=0.23753674274969097\n",
      "Current iteration=8700, loss=0.23721674989849176\n",
      "Current iteration=8800, loss=0.23691216628912373\n",
      "Current iteration=8900, loss=0.2366240987883002\n",
      "Current iteration=9000, loss=0.2363519451952456\n",
      "Current iteration=9100, loss=0.23609408371123727\n",
      "Current iteration=9200, loss=0.23584844931617202\n",
      "Current iteration=9300, loss=0.23561352205808403\n",
      "Current iteration=9400, loss=0.2353894644861581\n",
      "Current iteration=9500, loss=0.23517644231717133\n",
      "Current iteration=9600, loss=0.2349735669208068\n",
      "Current iteration=9700, loss=0.2347804490494792\n",
      "Current iteration=9800, loss=0.2345958653668334\n",
      "Current iteration=9900, loss=0.23441918906626452\n",
      "Accuracy for training Set :  0.9126921846191354\n",
      "Accuracy for testing Set :  0.9118045926219391\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.random.rand(tx.shape[1])\n",
    "w,loss = logistic_regression(yb_train, tx_train, initial_w , max_iters, gamma)\n",
    "print( \"Accuracy for training Set : \",accuracy(yb_train, predict(tx_train,w))) \n",
    "print( \"Accuracy for testing Set : \",accuracy(yb_test, predict(tx_test,w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=57.45301038142266\n",
      "Current iteration=100, loss=7.564112773603367\n",
      "Current iteration=200, loss=1.2921201519958885\n",
      "Current iteration=300, loss=0.671321428818382\n",
      "Current iteration=400, loss=0.6179704914731943\n",
      "Current iteration=500, loss=0.6130815467699791\n",
      "Current iteration=600, loss=0.6125934177883855\n",
      "Current iteration=700, loss=0.6125408918357547\n",
      "Current iteration=800, loss=0.6125349106561933\n",
      "Current iteration=900, loss=0.6125342012925209\n",
      "Accuracy for training Set :  0.8766094747588645\n",
      "Accuracy for testing Set :  0.8757218827616682\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "\n",
    "initial_w = np.random.rand(tx.shape[1])\n",
    "w,loss = reg_logistic_regression(yb_train, tx_train,0.5, initial_w , max_iters, gamma)\n",
    "print( \"Accuracy for training Set : \",accuracy(yb_train, predict(tx_train,w))) \n",
    "print( \"Accuracy for testing Set : \",accuracy(yb_test, predict(tx_test,w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ids, input_data, ids_test \u001b[39m=\u001b[39m load_csv_data(\u001b[39m\"\u001b[39m\u001b[39mdata/dataset/x_test.csv\u001b[39m\u001b[39m\"\u001b[39m, sub_sample\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yacine/Desktop/ML_course/projects/project1/main.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "ids, input_data, ids_test = load_csv_data(\"data/dataset/x_test.csv\", sub_sample=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
